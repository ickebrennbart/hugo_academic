[{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3207d675b7a60ab7d95d8d723ea47d76","permalink":"https://ai.kuleuven.be/stories/author/aras-yurtman/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/stories/author/aras-yurtman/","section":"authors","summary":"","tags":null,"title":"Aras Yurtman","type":"authors"},{"authors":null,"categories":null,"content":"I am motivated by exploring topics in the larger context they are embedded in and by creating interesting synergies at the intersection of technology and society. This led me to Leuven.AI where I focus on the strengthening and articulation of interdiscplinary and collaborative AI projects between our members as well as in collaboration with industrial and societal partners. I consider Leuven.AI stories as an effort to make our AI research at Leuven.AI more relatable and thus more meaningful to a wide range of people interested in technological developments in AI.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"fb6b355779d16f660168dbd08a7bca7b","permalink":"https://ai.kuleuven.be/stories/author/jens-burger/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/stories/author/jens-burger/","section":"authors","summary":"I am motivated by exploring topics in the larger context they are embedded in and by creating interesting synergies at the intersection of technology and society. This led me to Leuven.","tags":null,"title":"Jens Bürger","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3cf1e2a9d4302b2018f3dea0f0804398","permalink":"https://ai.kuleuven.be/stories/author/jessa-bekker/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/stories/author/jessa-bekker/","section":"authors","summary":"","tags":null,"title":"Jessa Bekker","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"57e3bb9f91bebe811a14d4d43cdfd0da","permalink":"https://ai.kuleuven.be/stories/author/jonas-soenen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/stories/author/jonas-soenen/","section":"authors","summary":"","tags":null,"title":"Jonas Soenen","type":"authors"},{"authors":null,"categories":null,"content":"Koen Vanthournout received his Master degree in Electrical Engineering (1999) from the Groep T Hogeschool of Leuven, Belgium, and his Master degree in Artificial Intelligence (2000) from K.U. Leuven, Belgium. He obtained a PhD in Electrical Engineering from K.U. Leuven in 2006. In 2020 he obtained the Postgraduate certificate School of Thinking from the Vrije Universiteit Brussel. From 2006 to 2009, he worked as a senior embedded software engineer for Icos Visions Systems/KLA-Tencor, after which he joined VITO, the Flemish Institute for Technological Research, and EnergyVille. In 2017, he co-founded ENION, a service provider for custom battery systems for SME\u0026rsquo;s, after which he returned to VITO and EnergyVille. He is currently working on smart grids, demand response, and the management of distribution grids.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2fd8f723cdf51cc651340fe128f3203c","permalink":"https://ai.kuleuven.be/stories/author/koen-vanthournout/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/stories/author/koen-vanthournout/","section":"authors","summary":"Koen Vanthournout received his Master degree in Electrical Engineering (1999) from the Groep T Hogeschool of Leuven, Belgium, and his Master degree in Artificial Intelligence (2000) from K.U. Leuven, Belgium. He obtained a PhD in Electrical Engineering from K.","tags":null,"title":"Koen Vanthournout","type":"authors"},{"authors":null,"categories":null,"content":"I worked as an Electronic Hardware Technician from 2008 to 2011 and as a Numerics Software Developer for Spatial and Spatio-Temporal Machine Learning on Geographic Information Systems from 2013 to 2018. I am currently working on time series analysis and modelling in data science application areas, including health 2.0 and industry 4.0 environments. My dream is that one day we will be able to create beneficial, ethical and explainable Artificial General Intelligence (AGI).\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ae87c482fbb7981eb1b4d49488a36bbc","permalink":"https://ai.kuleuven.be/stories/author/konstantinos-theodorakos/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/stories/author/konstantinos-theodorakos/","section":"authors","summary":"I worked as an Electronic Hardware Technician from 2008 to 2011 and as a Numerics Software Developer for Spatial and Spatio-Temporal Machine Learning on Geographic Information Systems from 2013 to 2018.","tags":null,"title":"Konstantinos Theodorakos","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3ecb4db705e98ec425d4289e6ae9aab6","permalink":"https://ai.kuleuven.be/stories/author/lola-botman/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/stories/author/lola-botman/","section":"authors","summary":"","tags":null,"title":"Lola Botman","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b22e9dd638143c383b865b46fcbf4e6c","permalink":"https://ai.kuleuven.be/stories/author/matthias-de-lange/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/stories/author/matthias-de-lange/","section":"authors","summary":"","tags":null,"title":"Matthias De Lange","type":"authors"},{"authors":["Jonas Soenen","Lola Botman","Konstantinos Theodorakos","Dries Van Daele","Aras Yurtman","Jessa Bekker","Koen Vanthournout"],"categories":["Machine Learning \u0026 Data Science"],"content":"The Flanders AI Program, started by the Flemish government, focusses on AI research, implementation, ethics and education. With this program, the Flemish government wants to ensure that Flanders is ready for the AI evolution. The whole program represents a yearly budget of 32 million euros. The research portion of the program (The Flanders AI Research program) aims to strengthen the fundamental research that is already present in Flanders and encourages researchers to apply AI techniques to several use cases in healthcare, Industry 4.0 and government. The program consists of around 120 researchers and has already resulted in more than 100 publications in high quality journals and conferences. In this post, we explore the low voltage grid use case of the program and give a glimpse of the ongoing research.\n  There is an immense energy transition underway as we move from fossil fuels to more renewable energy sources. More and more households become what we now call prosumers: they both consume and produce electricity (e.g. solar panels). In the past, electricity only flowed from the distribution grid to the household, but now, for prosumers, electricity flows from and to the household. In the meantime, the widespread adoption of electrical vehicles (EVs) is underway. The Belgian government is fostering this adoption: by 2026, they want emission-free company cars to be the norm and they are offering tax incentives for the installation of EV charging infrastructure.\nThis ongoing transition influences the way we use electricity. The widespread adoption of EVs, among other factors, is expected to cause a significant increase in electricity demand. Not only do we consume more electricity, we also consume differently, i.e. the electricity consumption patterns are changing. For example, on a sunny day, a neighborhood with lots of solar panels can generate significantly more electricity than what is consumed. Each household with solar panels injects its electricity surplus into the grid at the same time, causing a high simultaneous injection load on the distribution grid. Analogously, when people return from work, they plug in their EVs to charge. In some places with lots of EVs, this might again result in a high simultaneous load on the grid.\nTo meet the changing electricity consumption, the low voltage (LV) distribution grid will need to be reinforced in the near future and doing so cost efficiently is a major challenge. The electrical grid as we know it today was built over the course of the last century using a ‘fit and forget’ strategy: install over-dimensioned cables with sufficient capacity to cover all peaks in electricity demand. Historically, this was a very efficient method as the peak on a cable was typically only 20-30% of the sum of individual peaks of the houses connected to the cable: we do not have our individual highest consumption at the same time. However, as our consumption increases and becomes more synchronized, there is a higher risk that at some point the installed capacity doesn’t suffice and congestion starts to occur. That is, the voltage drops too low or rises too high, leading to appliance malfunctions, and ultimately too high currents blow the cable fuse and the lights go out. Upgrading the grid using the ‘fit and forget’ strategy to handle higher loads would be expensive. The scale of the LV distribution grid is massive; Flanders alone counts 84000km of cable and appr. 3,5 million connection points. To reinforce mere percentages already runs into the hundreds of millions Euro. The main challenge is to support the increasing load on the low voltage grid through a more optimal use of the capacity of the current grid assets and targeted grid reinforcement investments only where necessary.\nThis is one of the challenges that EnergyVille (a cooperation between KU Leuven, VITO, imec and UHasselt) is tackling in collaboration with Fluvius, the Flemish distribution system operator (DSO).\nIncreasing the visibility in the low voltage grid The first step towards more cost-effective grid management is to gain a detailed and accurate view on the current state of the LV grid. Obtaining more detailed information about the grid’s layout and state allows us to identify the problematic parts of the grid and to quantify the severity of these problems.\nTo achieve this, EnergyVille is developing ‘digital twin’ capabilities for the LV grid: a set of tools that yields a more detailed view on the grid by applying a combination of AI and Power System Engineering techniques on the available data. One of the central systems in this toolset is a simulation environment that, given the consumption of connected households and the layout of the LV grid, calculates all currents and voltages in the grid (i.e. how the electricity ‘flows’ through the grid). However, consumption measurements in the LV grid - although increasing - remain sparse and our knowledge about the layout of the grid is incomplete and often inaccurate. Therefore, a crucial aspect of the work is to develop tools that correct and enhance the available data.\n  As one of the industrial use cases within the Flanders AI Research program, a collaboration was set up between DTAI-KU Leuven and STADIUS-KU Leuven research groups and EnergyVille to tackle some of the data quality issues and to take the first steps towards predicting problems in the grid ahead of time:\n How can we generate accurate consumption data for households with no or only few available consumption measurements to use in the grid simulation? Can we derive accurate maps of the low voltage grid based on available schematic drawings? Can we forecast congestion problems in the grid ahead of time? Can we reduce the time it takes to simulate the grid for a longer period of time, e.g., a year?  Measuring the similarity between households and clustering them To be able to simulate the grid, the system developed by EnergyVille needs quarter hour consumption data for every household. However, only for a small portion of households there is such consumption data available for a longer period of time. As such, the goal of this task is to accurately measure the similarity between different consumption profiles. Whenever a profile of a certain household doesn’t have enough data available, we can use consumption data of the most similar complete profiles instead.\nThe biggest challenge when measuring the similarity between energy consumption profiles is that the profiles themselves are very irregular and highly stochastic. For example, one day, a household might cook extensively using the stove, and on another, they may simply use the microwave. The exact time at which they cook might also differ from day to day. As such, comparing the corresponding timestamps of two profiles (e.g. the consumption measured on Jan 1 at 19:15 of profile 1 with the consumption on Jan 1 at 19:15 of profile 2) might consider two profiles highly dissimilar while both profiles actually behave similarly: they cook around the same time (e.g. plus/minus 30 minutes) and both use the stove or microwave to cook (but not always on the same day). Hence, we aim to develop a flexible similarity measure that takes the stochasticity of electricity consumption into account.\nBased on such a flexible similarity metric, we can apply COBRAS, a clustering algorithm developed at DTAI-KU Leuven. COBRAS takes into account the domain experts knowledge by posing questions to the expert during the clustering process. Embedding this knowledge in the process ensures that the produced clustering is suited for the problem at hand. The resulting clustering allows EnergyVille to study the different types of energy consumers in Belgium and provides an easy way to sample consumption data to use for simulating the grid.\nFor more information see the COBRAS webpage or a gentle introduction into using COBRAS for time series.\nDeriving accurate maps of the low voltage grid from schematics In addition to consumption data, a simulation of the low voltage grid also requires an accurate map of the grid layout. It is crucial to know which houses are connected to which cables; when many houses attached to the same part of the grid simultaneously experience a peak in energy consumption, that part of the grid is at risk for congestion. Furthermore, to accurately calculate the voltage drop or rise , the length of the cables is required to be known.\nUnfortunately, only inaccurate schematic drawings of the LV grid are available. These schematics were historically intended for human interpretation. Because of this, they display inaccuracies that can generally be resolved using common sense. When multiple cables run alongside the same side of the road, they are depicted as visually distinct lines, which greatly exaggerates their true distance from one another. Near transformers, where many cables gather, the depiction becomes particularly chaotic. Furthermore, important information such as the knowledge of which houses are connected to which cable is left implicit, to be inferred by the reader. An unfortunate side-effect of these visual simplifications and abstractions is that naive, automated usage of this schematic data is likely to result in some invalid beliefs about the low voltage grid layout.\nSome example synthetic data as it might appear in a schematic, where the house-to-cable connections have been manually added.   We are developing novel methods to enhance the available schematic drawings and to derive a more accurate map of the low voltage grid with correct house-to-cable connections. To capture the real-world context with its numerous roads and houses, we enrich the schematics with public geographical information from Geopunt Vlaanderen. While all of this is raw geographical (GIS) data consisting of simple points and lines in a 2D-space, the challenge is to facilitate learning models that capture the visual common sense that underlies the schematics. This is achieved by introducing novel geographical primitives that capture the visual relations between objects, rather than just their coordinates. Examples of such primitives are: “points A and B are on the same side of line C” or “there is no obstruction between point A and B”. Machine learning methods can build upon these concepts to learn more complex and qualitative models, without having to deal with the raw data.\nForecasting households’ consumption 24 hours ahead One way to forecast congestion problems in the LV grid is to forecast each connected households’ consumption and feed this forecasted consumption into the simulation environment. If the forecast indicates that there might be congestion tomorrow, action can be taken in an effort to prevent it (for example, sending appropriate control signals to devices such as smart heat pumps, smart charging of EVs, home batteries, etc. in order to change their load).\nHowever, as mentioned in the clustering task, the consumption profiles are very irregular and stochastic and therefore difficult to forecast. Energy consumption profiles are usually characterized by sudden short high electrical consumption periods, appearing as peaks in the consumption profiles. To forecast congestion accurately it is essential to know when a demand peak will take place and how large the peak will be. However, conventional regression algorithms such as Auto Regressive Integrated Moving Average (ARIMA) and Long Short Term Memory (LSTM) networks struggle to predict these peaks because the input data is highly imbalanced (i.e. lot of low values and only a few high values).\n  To sidestep the issues around the imbalance of the input data, we reframe the forecasting (regression) problem as a classification problem. In a regression problem, the algorithm will predict the next value, while in a classification problem, the algorithm will predict whether or not the next value is a peak value. In the reframed problem there will still be significantly more lower values than peak values but in a classification setting there are several approaches to resolve this imbalance. In the next phase, to also predict the size of the peak, we could opt for a multi-class classification problem where a peak value is also classified as low peak, high peak, very high peak, etc… This change in perspective is currently being explored and, if successful, would allow us to increase the congestion prediction performances.\n  Robust days-of-year clustering of smart-meter data As discussed earlier, grid simulations can support distribution system operators in making informed long term strategic decisions. However, performing all the required calculations that simulate the intricate parts of the low voltage grid takes time. Simulating the grid for short periods of time is feasible (e.g. a single day of simulation requires around 14 hours of computation), but simulating for longer periods of time (e.g. a year) becomes problematic in terms of the calculation time. While single-day simulations are useful in assessing specific “worst-case estimates”, full-year simulations are necessary to get a good overall idea about the frequency and severity of congestion problems.\nLuckily, there might be a way around this problem. A year contains days that are similar to each other, both meteorologically and in energy production/consumption, e.g.,days in the same season, weekends/holidays. As such, instead of simulating every day of the year, we can perform the simulation for a limited number of representative days. This strategy reduces the calculation time of a full-year simulation considerably. However, in order for this to work, we need a way to select the representative days such that the quality of the reduced simulation is as close as possible to the difficult-to-calculate full-year simulation.\n  To select the representative days, we cluster the days of the year based on household electricity consumption and meteorological (e.g. temperature, rainfall, …) time series data; the medoids (i.e. centers) of each cluster are selected as representative days. After preprocessing the timeseries to remove erroneous values and impute missing values, matrix decomposition reduces the dimensionality of the problem and robust k-medoids performs the clustering itself. The (hyper) parameters of the matrix decomposition and the k-medoids algorithm are optimized through bayesian optimization. A schematic overview of our approach is shown in the figure above.\nThe low voltage grid: from barrier to enabler of the energy transition Without realizing it, we use the LV grid every day; our houses, offices, and schools are connected to it. Each time we leave or enter a building, we pass over or under its myriad of cables. It is a critical infrastructure without which our civilization would fail to function. The various disaster books and movies on nationwide blackouts attest to this. Yet we rarely think of the grid. We have almost forgotten that it is there.\nThis seemingly boring collection of copper and aluminum wires finds itself at the forefront to stop climate change. Renewable electricity is the most important alternative to fossil fuels. This electrical ‘fuel’ that powers our cars and heating systems must now pass over the LV grid. Heating a house with a heat pump and charging the owner’s electrical car roughly triples the household’s electrical consumption and on a sunny day that same house injects its excess local production into the grid. Handling this increased consumption and injection is a formidable challenge that shakes the system. Not in the future, but already today; simply check a random newspaper to see the many articles discussing electric cars, support systems for solar panels, cost of charging infrastructure, etc. The grid is no longer invisible.\nAnd so, that is our goal: to help make the LV grid invisible once more. To ensure that we have the technology to minimize the cost of upgrading the grid to support renewable electricity production and the electrification of fossil fuel consumers. To ensure that we have all the data, forecasts and control signals such that our shiny new green devices can respond automatically to the availability of renewable energy while respecting the transport capacity of the local grid. To ensure that the LV grid is no bottleneck for the energy transition but an enabler of it. So we can continue to live our lives in comfort, totally oblivious of what is behind those power sockets in the wall.\n","date":1635897600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635897600,"objectID":"60537fd539c3b00eefbeb88f0b9961eb","permalink":"https://ai.kuleuven.be/stories/post/2021-11-03-energy-forecasting/","publishdate":"2021-11-03T00:00:00Z","relpermalink":"/stories/post/2021-11-03-energy-forecasting/","section":"post","summary":"Increasing household electricity consumption (e.g. widespread adoption of electrical vehicles) is driving the low-voltage energy grid to its capacity limits. Simply replacing or scaling up such an infrastructure is neither trivial nor cost-effective. Instead, the Flanders AI Research Program, supports research on intelligent solutions for grid management. Here we present different AI-driven approaches currently under investigation by KU Leuven and EnergyVille.","tags":["energy forecasting","smart grid"],"title":"Enabling the energy transition with smart meter data","type":"post"},{"authors":["Matthias De Lange"],"categories":["Language, Speech \u0026 Vision"],"content":"Machine learning (ML) is a type of Artificial Intelligence (AI) where machines can learn from huge amounts of data, such as images or text, containing highly complex patterns. Previously, these patterns had to be discovered based on hand-crafted rules of thumb, whereas today the horse-power of machine learning algorithms resides in discovering these patterns automatically. Many contemporary successes are mainly thanks to the resurgence of a specific machine learning algorithm, called artificial neural networks. These neural networks excel in a wide range of relevant tasks, ranging from detecting pedestrians and cars in videos, to generating convincing articles, and even greatly facilitating drug discovery by predicting protein structures. Traditional machine learning typically first learns a model using all the available examples in the data and is then deployed for real-world use. This means that whenever a model has finished learning, it remains unchanged when used in practice. The model\u0026rsquo;s static nature is problematic as it doesn\u0026rsquo;t befit our ever-changing world. For example, with the rise of autonomous vehicles and the internet of things (IoT), data becomes available at previously unprecedented rates and continuously changes over time. This leaves many contemporary machine learning approaches in the dark, as the static model at deployment can\u0026rsquo;t put the never-ending flow of data to use. Continual Learning aims to tackle this problem by taking a closer look at dynamic machine learning models, adapting to the data flexibly and continuously.\nOvercoming a Catastrophe Let\u0026rsquo;s consider an object detection system for a self-driving car. For safe operation, we give our car the task to detect pedestrians and cyclists. By training our neural network with a large number of samples, it does a remarkable job. However, new object categories emerge all around, such as hoverboards and electrical steps. If we were to update our detection system now with a host of corresponding images for this new task, again it would do a remarkable job at detecting hoverboards and electrical steps. But there is a catch: our system would completely forget about the pedestrians and cyclists in the old task, hence leaving them undetected! This is a phenomenon known as catastrophic forgetting in neural networks.\nContinual Learning explores how to learn sequentially without this catastrophic forgetting, such that the neural network remembers all learned concepts over time. The most obvious way to remember what is learned from the data, is by simply storing it all and revisiting it later whenever we are about to learn a new task. However, we have to take into account that these data streams might be never-ending, which doesn\u0026rsquo;t abide with the certainly-ending memory in our machines. Therefore, it is paramount to overcome catastrophic forgetting within the predefined memory constraints of our system.\nA body of Continual Learning methods has been proposed to deal with this issue and can be mainly divided within three families. The first family of regularization methods typically constrains connections between neurons to remain unchanged when they are deemed to be important for an old task. The importance for each connection is tracked to urge new tasks to learn only with connections irrelevant to the old tasks, which greatly alleviates catastrophic forgetting. Secondly, the family of parameter isolation methods takes another approach by directly allocating a part of the network to each task. As each task is only able to alter its own part of the network, the other tasks can\u0026rsquo;t interfere when learning a new task. Finally, the family of replay methods stores a small part of the seen data, which is then reused later while learning a new task. It\u0026rsquo;s a simple approach but with some additional benefits, which we\u0026rsquo;ll explore in more detail later in this article! But first, we need to explain how neural networks learn and represent the world.\nRepresenting the world Whenever we think about a concept, say a donut, we can imagine such delight in many varieties, differing in properties such as color and size. Even more so, if we would encounter a sugarcoated torus-shaped pastry in a color we\u0026rsquo;ve never seen before, we are still able to allocate it to this same semantic concept. This is because our human brains often use representative heuristics, where each concept is represented by a most representative prototype.\nNeural networks are built by multiple consecutive layers of neuron-like units, remotely based on neurons in the human brain. Typically, many consecutive layers are used, which is why it is often referred to as deep learning. This hierarchy of layers allows building increasingly complex representations. In case of the donut example, early layers would have some low-level processing like the curved lines and the color, whereas later layers combine these properties to form the representation of the donut. If we now observe a lot of donut images, we can combine all of their representations to make the donut prototype. You can think of this prototype as the most generic, plain donut any other donut would resemble.\nEventually, we want such a representative prototype for each of the concepts in our data. In fact, these prototypes capture the most important characteristics of the related concept so well, that when we give a new image to our network it has never seen before (e.g. take a picture with your smartphone), it could tell you which concept\u0026rsquo;s prototype it corresponds to (you took a picture of a donut). This predicting of unseen images is called the testing phase, and can be visualized as in the following:\nIn the testing phase, the neural network takes in a new data sample, such as an image of a donut, and produces a representation containing the sample\u0026#39;s characteristics. By matching this representation to the prototypes of the concepts, we can predict the concept with the most similar prototype, which is in this case ideally the donut.   -- But in order to obtain these prototypes, we need the neural network to learn in some way from the images it gets presented, this process is called the training phase. First of all, the neural network needs an indication of the concepts in an image, such that its corresponding prototype can be identified. Typically, a human supervisor comes into play, annotating each image with a label, indicating it contains one or the other concept, and hence indicating the prototype each image belongs to.\nNow that each image is associated with a certain prototype, we can train the neural network by means of an error function. When the representation of the image ends up far from the prototype, we tell the network to make changes in its connections between the neurons, such that the representation becomes more similar to its prototype, and matches less with the other concepts' prototypes. This standard error in machine learning is called the Cross-Entropy error, but has severe limitations when used for continual learning. As we only see images from concepts in our current task, the prototypes of the old task\u0026rsquo;s concepts are uncontrollably changed, such that they are no longer the most generic representation. In other words, the old prototypes lose their ability to serve as a good approximation to detect future instances of their related concept. Hence, the old task concepts are prone to catastrophic forgetting. This indicates that for Continual Learning, training clearly needs an alternative for this Cross-Entropy error, maintaining the quality of our prototypes.\nEvolving prototypes Recent work in the lab of Prof. Tinne Tuytelaars addresses the problem of deteriorating prototypes by means of Continual Prototype Evolution, referred to as CoPE. The primary goal of this Continual Learning approach is to keep the prototypes for all concepts up-to-date at all times. As the knowledge about our previous concepts resides within our adaptive prototypes, we don\u0026rsquo;t need to store all data for retraining as in standard machine learning approaches. Therefore, if we would determine to add images of new concepts, such as pretzels, to our collection of pastry images, we don\u0026rsquo;t need to start retraining the neural network all over again, but we can learn from them much like we humans do, without catastrophic forgetting.\nCoPE belongs to the family of replay methods, which means it stores a few images from the old tasks in a small buffer. When we learn now from the new pretzel images, we mix them with those from the small buffer to give our neural network the illusion that we actually had all pretzel and other pastry data available at once.\nIn fact, the training phase in CoPE can be divided into two stages. First, it refreshes the prototypes and then uses them to learn better representations. This addresses a first problem with the previously discussed Cross-Entropy error, which updates the prototypes and representations at the same time. In contrast, CoPE first updates the prototypes with the available representations of the new task images mixed with images from the small replay buffer. Only then, the neural network itself is improved to get better representations, while already exploiting the improved prototypes. This can be seen as a type of look-ahead, where the prototype of better quality is used immediately instead of in the next update.\n1. Update Prototype The donut prototype is slightly changed towards the representation of the donut image. This makes the prototype better in representing all the donut images.   *The donut prototype is slightly changed towards the representation of the donut image. This makes the prototype better in representing all the donut images.* --  2. Update representation using the prototypes The neural network connections are changed to make the representation of the donut image more similar to its prototype and more dissimilar to other concept prototypes such as the croissant prototype.   *The neural network connections are changed to make the representation of the donut image more similar to its prototype and more dissimilar to other concept prototypes such as the croissant prototype.* -- A second problem in the Cross-Entropy error is that it blindly changes all the other concept prototypes, leading to catastrophic forgetting of old concepts. To tackle this issue, CoPE proposes a new Pseudo-Prototypical Proxy error function (PPP error), which doesn\u0026rsquo;t alter any of the prototypes. Instead, it updates only the representations. This PPP error makes the representations of a concept more similar to their prototype, and decreases similarity to the other concept prototypes. This is reminiscent to contrastive error functions often used in representation learning. However, in the PPP error, the representation of each image also acts as a pseudo-prototype for the other representations being processed. Basically, this means that when multiple images of the same donut concept are being processed, the representation of one image will also be used for the other as a stand-in of the donut prototype. This gives incentive for all images of the same concept to become similar to their prototype, reducing the chance that they end up matching better with a wrong prototype.\nUsing pseudo-prototypes in the PPP error function:\nThe representation of the second donut image is updated to better match the donut prototype, but also the representation of the first donut image, which acts as a pseudo-prototype.   *The representation of the second donut image is updated to better match the donut prototype, but also the representation of the first donut image, which acts as a pseudo-prototype.* -- In conclusion, mixing the next task data with a small replay buffer, combined with addressing these two problems in the Cross-entropy error, shows to have striking improvements over other replay methods when learning from data streams.\nThis introduction to the surging field of Continual Learning and one of its methods are part of a much larger effort. Prof. Tuytelaars' lab has worked on many other aspects of continual learning, among which a fundamental study of replay, regularization methods, task-free continual learning, scalable user adaptation, and an extensive survey. Furthermore, her lab is active in the ContinualAI community, maintaining a wiki, open-source code, and workshops that make Continual Learning easily accesible for everyone, and where you can continue your journey if you can\u0026rsquo;t get enough of this topic. Note that we are not responsible for catastrophic forgetting upon reading this article.\nAn illustration of catastrophic forgetting in neural networks. Cartoon credits @Jasper De Lange.   *An illustration of catastrophic forgetting in neural networks. Cartoon credits @Jasper De Lange.* -- ","date":1620604800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620604800,"objectID":"ca1b736e056168625b086591dea9e6d1","permalink":"https://ai.kuleuven.be/stories/post/2021-05-10-continual-learning/","publishdate":"2021-05-10T00:00:00Z","relpermalink":"/stories/post/2021-05-10-continual-learning/","section":"post","summary":"A remaining challenge to many machine learning approaches is the divide between training and deployment of models, limiting their relevance to either very narrow application domains or to short periods of time before new data renders them outdated. In this post, we are presenting state-of-the-art work on Continual Learning, a stream of ML research aiming to make ML models truly adaptive.","tags":["continual learning","computer vision","adaptive AI"],"title":"Towards Adaptive AI with Continual Learning","type":"post"},{"authors":["Peggy Valcke","Nathalie Smuha","Eyup Kun"],"categories":["PhELSI"],"content":"Increased awareness of AI’s risks when left unchecked – from discrimination to surveillance – has driven regulators across the world to work on AI-specific rules. Last year, the European Commission launched a public consultation about its White Paper on Artificial Intelligence published in February 2020, to which also Leuven.AI and CiTiP jointly responded. This year, on 30 March 2021, the Council of Europe’s CAHAI launched a call inviting stakeholders to provide feedback about its Feasibility Study1 and important regulatory choices to take with regard to artificial intelligence.\nTime to find out more about what kind of organisation the Council of Europe is, what it is doing in the area of AI, and why it is important to take part in the discussion! CiTiP’s Eyup Kun had an interview with Leuven.AI members Peggy Valcke and Nathalie Smuha about the Council of Europe and its activities in relation to AI.\n  Peggy Valcke    Nathalie Smuha    Eyup Kun    Peggy, Nathalie, maybe we should start with what exactly is the Council of Europe and what is its role in relation to digital technologies such as AI?  Peggy: The Council of Europe is not to be confused with the European Council. The latter is an institution of the European Union, the economic and political union of 27 Member States on the continent. The former is actually Europe\u0026rsquo;s oldest political organisation and stands separate from the European Union; it was founded in 1949, in the aftermath of the Second World War. The Council of Europe’s mission is to protect human rights, democracy, and the rule of law in Europe. It currently consists of 47 member States (all Member States of the European Union, and other neighbouring countries such as the United Kingdom, Iceland, Liechtenstein, Norway, Russia, Armenia and Turkey). On the basis of discussion and collaboration among its member States, the Council of Europe can establish binding and non-binding legal instruments. Given that it is an international organisation, the binding nature of its legal instruments stems from states' voluntary commitment to adopt, ratify, and transpose the treaties into their national law. Unlike the European Union, the Council of Europe cannot adopt legal rules that have immediate binding legal force throughout every Member State.\nNathalie: The Council of Europe and its bodies have a significant impact on regulating the digital world from the standpoint of human rights. Firstly, The European Court of Human Rights considers the European Convention on Human Rights (ECHR) as a living instrument. Although the Convention was established long before the digital revolution, the Court interprets it in a way that ensures consistent protection of human rights among its member states and applies these rights to the context of today. This type of evolutive interpretation allows the Court to include the “right to data protection” as part of the “right to a private life” under Article 8 of the Convention, considering the objective and values embedded under that Article. Also the case-law of the Court on freedom of expression and access to information under Article 10 of the Convention continues to have an effect on the regulation of the digital media and access to the Internet among member States.\nIn addition, in the digital field, the Council of Europe succeeded in the first binding international convention on data protection (Convention 108), which was opened for signature in 1981. This Convention and its modernised version, Convention 108+, aim to ensure safeguards for the protection of personal data and the right to privacy against the challenges of information technologies by providing high-level principles, including data minimisation, transparency and accountability, and by equipping individuals with data subject rights. With its non-binding instruments, the Committee of Ministers adopted the recommendation on human rights impacts of algorithmic systems to the member States. The Consultative Committee of Convention 108 also provides specific guidelines on new regulatory challenges, such as recent guidelines on facial recognition technology. The Budapest Convention on Cybercrime is another example of how the Council of Europe acts as a global standard-setting actor in the digital field.\n You are both very active within the Council of Europe’s CAHAI – what is CAHAI and what is precisely your role?  Peggy: The Ad hoc Committee on Artificial Intelligence (CAHAI) was set up in September 2019 by the Committee of Ministers of the Council of Europe (one of its two governing bodies, besides the Parliamentary Assembly). It was tasked with the mission to examine the feasibility of a legal framework for AI systems (henceforth, Feasibility study) in line with the Council of Europe’s standards on human rights, democracy and the rule of law (see CAHAI’s terms of reference). It was stressed from the beginning that CAHAI would have to complement, and not duplicate, ongoing legislative efforts both within and outside the Council of Europe. So one of the first tasks that CAHAI took up was an exhaustive mapping of the various initiatives by other international and regional organisations, such as the European Union, the OECD, UNESCO, etc., as well as within the various Council of Europe bodies. CAHAI also collected information about relevant national initiatives. All this information is made available via CAHAI’s website (see here and here; also keep an eye out for the joint website that will show all initiatives at the global level).\nShortly after the launch of the Flemish Expert Centre on Data \u0026amp; Society, I was invited by the Flemish authorities to represent Belgium at CAHAI, together with a colleague representing Wallonia. During CAHAI’s first plenary meeting in November 2019, I was elected vice-chair, meaning that I also take part in bureau meetings, have regular contact with CAHAI’s secretariat and follow the various working groups set up within CAHAI from close by.\nNathalie: Since Autumn 2020, I have served as an independent expert on AI policy and regulation to CAHAI. I provided expert guidance in the drafting process of the Feasibility Study, take part in plenary and working group meetings, and work closely with CAHAI’s secretariat for the activities of the Policy Development Group and Legal Frameworks Group. Prior to my involvement with the Council of Europe, I coordinated the work of the European Commission’s High-Level Expert Group on AI - an experience that has proven to be very useful in this context, which also evolves around AI policy.\n The Feasibility Study was adopted by CAHAI in December 2020 and marks an important milestone in its activities. What are important key take-aways?\n Nathalie: The feasibility study first touches upon opportunities and risks arising from the use of AI systems. In terms of opportunities, the study refers to the potential positive impact of AI in numerous domains, for instance by improving current industrial capabilities, the provision of healthcare and educational opportunities. Yet the scale of use of AI systems, and their opacity and complexity, can in some instances also amplify the risks inherent in human decision-making (like unjust biases for example) and cause new risks due to potential \u0026ldquo;vicious feedback loops”. The study therefore elaborates on how AI systems can impact human rights, democracy and the rule of law, taking the rights of the ECHR as a starting point.\nFor instance, AI systems can pose a threat to the right to liberty and security (Article 5), the right to a fair trial (Article 6) and the right to an effective remedy (Article 13), when opaque systems are used in situations where freedom or personal security is at stake. The fact that they can facilitate or amplify unjust bias also poses a risk to the right to non-discrimination (Article 14). Furthermore, the right to privacy (Article 8) entails not only freedom from surveillance, but also the protection and enhancement of human autonomy. Being monitored indiscriminately by AI systems can steer people to think and behave in a specific way, impacting their autonomy. Due to the intrusive nature of some AI systems, especially online platforms for instance, freedom of expression (Article 10) and freedom of assembly and association (Article 11) are not unaffected either. These are just a few examples. Many of these rights are also closely related to safeguarding the integrity of the democratic process and the rule of law, which can hence also be negatively impacted.\nIt’s however important to keep in mind that these risks depend on the application context, the type of technology used and the stakeholders involved. Not all AI systems pose the same type and level of risk.\n How does the Feasibility Study propose to address these risks? What does it consider to be “main elements of a legal framework” for AI-systems?  Peggy: After analysing the benefits and risks of using AI systems, the feasibility study identifies principles that should be horizontally (i.e. irrespective of the sector or context) applicable to the design, development and use of AI systems. These principles are, among others, human dignity, human freedom and autonomy, non-discrimination, gender equality, fairness and diversity, accountability and transparency. To operationalise these principles, the study matches them to corresponding concrete rights and obligations. The rights foreseen under the feasibility study are drawn from existing rights (the right to respect for private and family life), new rights tailored to challenges of AI systems (right to a meaning explanation of how AI system works) and clarification of existing rights (the right to physical, psychological and moral integrity in light of AI-based profiling and affect recognition). The feasibility study proposes how these obligations can be allocated to public and private actors, considering their uses and the different phases of AI application.\nIn addition to setting principles and corresponding rights and obligations, the feasibility study, similar to the work of the High-Level Expert Group on AI (set up by the EU Commission), recommends a risk-based approach towards the regulation of AI systems. It highlights that the risk of AI systems varies depending on its context and sector used. A risk-based approach will help member states contextualise and set the risk level of AI systems in the different uses and provide tailored mechanisms for various risks identified. Where needed, the study also recommends that a more prudent precautionary approach (including potential prohibitions) should be considered.\n What are the options for the Council of Europe Legal Framework according to the feasibility study?  Nathalie: After listing relevant principles, and potential rights and obligations, the feasibility study discusses the potential form of a future legal framework. The study considers the possibility of adopting binding and non-binding legal instruments and how they complement each other. It investigates the possibility of adding an extra protocol to the existing European Convention on Human Rights or to Convention 108+ on privacy, and the option to adopt a new binding convention. Both the advantages and possible shortcomings of all these options are explored, with the conclusion that mere voluntary guidelines are insufficient to protect people against AI’s risks.\nA legal framework should therefore include both legally binding and non-binding instruments to provide comprehensive protection and guidance, taking the specificities of different sectors and the risks and benefits of AI into account. Equally important, the study recommends practical mechanisms for compliance with the future legal framework, such as human rights impact assessments or auditing and certification mechanisms, as well as follow-up mechanisms and measures for international co-operation to guarantee the framework’s effectiveness.\n Why should organisations like Leuven.AI participate in CAHAI’s multi-stakeholder consultation?  Peggy \u0026amp; Nathalie: The feedback from the consultation will directly inform CAHAI’s Legal Frameworks Group which is preparing input for a possible European Convention on AI. It is crucial to collect different perspectives, which will feed the ongoing discussion on the regulation of AI systems. Leuven.AI consists of people from different backgrounds, including computer engineers, neuroscientists, lawyers, philosophers, mathematicians. The perspective they bring can have a real impact on how international regulation of AI systems should look like. It is important to understand that such regulation should not be seen as hampering innovation, but as a means to steer innovation in the direction of socially responsible and desirable outcomes. Probably many of you have seen the 1993 movie “Jurrasic Park”? In a famous scene, Jeff Goldblum, who plays a chaos theory expert, tells the owner of a dinosaur theme park that\n“your scientists were so preoccupied with whether or not they could [create dinosaurs from prehistoric DNA] that they didn’t stop to think if they should.” Let’s take the time to reflect together on which AI applications we should and should not develop together…\n -- Eyup Kun  ‌ ‌ ‌\n In response to the open consultation process, Leuven.AI will organise a discussion to prepare our feedback to the CAHAI proposal. We strongly encourage all Leuven.AI researchers wanting to contribute to future standards of AI regulations to join this discussion. More details to follow soon on Leuven.AI.      For an accessible introduction to CAHAI’s Feasibility Study, you can consult the Primer that the Alan Turing Institute and the Council of Europe just published. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1617321600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617321600,"objectID":"200dbfd7e599eeadac6f21428cd2eac6","permalink":"https://ai.kuleuven.be/stories/post/2021-04-10-cahai/","publishdate":"2021-04-02T00:00:00Z","relpermalink":"/stories/post/2021-04-10-cahai/","section":"post","summary":"Increased awareness of AI’s risks when left unchecked – from discrimination to surveillance – has driven regulators across the world to work on AI-specific rules. Time to find out more about what kind of organisation the Council of Europe is, what it is doing in the area of AI, and why it is important to take part in the discussion!","tags":["CITIP","regulating AI","council of Europe"],"title":"Europe as International Standard Setter for Artificial Intelligence: the role of the Council of Europe","type":"post"},{"authors":["Jesse Davis","Tom Decroos","Pieter Robberechts","Maaike Van Roy"],"categories":["Machine Learning \u0026 Data Science","Applications of AI"],"content":" This content is reposted with permission from the DTAI Sports Analytics Blog   Last year, we introduced our Player Vectors approach for automatically characterizing a player’s style of play. In the meantime, we haven’t been standing still and developed SoccerMix: an improved approach which takes account of the location and direction in which actions are performed. Beyond this, we have also introduced a methodology for exploring defensive style. Namely, our insight is that the objective of defending is often to prevent certain actions from occurring. Hence, some insights into a team’s defensive style can be found by measuring how a team forces its opponents to deviate from their usual playing style.\nIn this blog post, we describe SoccerMix and illustrate its applicability in three relevant use cases for the 2018/19 Premier League season:\n Player comparisons: How do the playing styles of Firmino and Agüero differ? Defensive analysis: How did Liverpool force its opponents to deviate from their typical style? Match analysis: Did Liverpool deviate from its usual style in the crucial away match against City in 2018/19?  Playing Style = Location + Action + Direction Previously, our working hypothesis in this area was that a player’s style manifests itself by:\n His or her preferred locations to occupy on the pitch, and Which actions he or she performs in each location  This hypothesis ignores one crucial piece of information. To illustrate that, consider two right-backs: Trent Alexander-Arnold and Kyle Walker. Alexander-Arnold is known for his creative and adventurous passing style. While sometimes risky, these passes have the potential to generate a lot of danger. In contrast, Walker tends to make shorter and more lateral passes. This difference in style is clearly visible in the passing maps for each player from one general location on the pitch:\nThese show that Alexander-Arnold and Walker tend to move the ball in very different directions from this location, which provides evidence that the direction players move the ball in from a location is another crucial aspect of playing style that should be added to our hypothesis.\nThe SoccerMix Approach The key challenge then is to come up with a succinct way to capture the action-location-direction relationship for each player. Previously, our Player Vectors approach used non-negative matrix factorization (NMF) to perform a location-based grouping of actions into a small set of prototypical or canonical actions. To illustrate this idea, the figure below shows the canonical shot actions that we identified.\nGrouping actions on both the location and direction poses several challenges. First, we need to be able to group actions based on both discrete (i.e., action type) and continuous properties. Second, we need to ensure that we group the actions on the right level of granularity. There should not be overly large clusters and similarly we do not want many groups with little or no actions in them. Finally, we need to be able to perform the groupings while accounting for the fact that locations and directions have different notions of similarity. In particular, NMF would struggle to cope with these last two challenges when forced to simultaneously consider both location and direction. This necessitates a fundamentally different approach to grouping actions.\nSoccerMix finds canonical actions by using mixture models to group together similar actions on the basis of their start location and direction. Mixture models differ from k-means in two important ways. First, they represent each cluster using a probability distribution instead of a centroid. Second, each object has a probability of belonging to each cluster instead of assigning the object to precisely one cluster. Hence, mixture models are often referred to as a soft version of k-means.\nSoccerMix uses the following two-step approach for each considered action type.\n First, it performs a location-based clustering of actions. For example, consider the passes made on the right third of the field by Kevin De Bruyne in the 2018/2019 season and one can immediately see some common starting locations. To automatically discover, such groupings, SoccerMix clusters the start locations of all actions using a mixture of Gaussian distributions.  Second, it performs a direction-based soft clustering of actions in each location group. To illustrate the intuition, let’s focus on one cluster of De Bruyne’s passes:  Here, it is possible to see that De Bruyne tends to play the ball backwards (shown in red), to the right flank (shown in blue) or in a probing, forward manner (shown in green). To capture directionality, we first use an action’s start and end location to derive its direction. Then, SoccerMix clusters all the actions in a location group using a mixture of Von Mises distributions, which are suitable for circular data. The use of Von Mises distributions are needed to describe backward passes. This is illustrated in the figure below, which shows the learned Von Mises distribution for passes that start in the central midfield. Unlike a Gaussian, a single Von Mises distribution can be fitted to passes close to 170° and -170°.\nThe end result of SoccerMix are a number of prototypical actions:\nWhat can you do with this analysis? SoccerMix can facilitate a number of interesting analyses, both on the player and team level. Here we will focus on three canonical cases. On the player level, we will show how to compare and contrast the playing style between two players. On the team level, we compare how often a team’s behavior differs in a specific match versus their typical behavior in all other matches.\nPlayer comparisons: How do the playing style of Firmino and Agüero differ? Sergio Agüero (Manchester City) and Roberto Firmino (Liverpool) are two world-class strikers. However, they approach the position differently. Agüero’s playing style is more like an out-and-out striker who most dominantly operates around the opponent’s penalty box. Firmino on the other hand is a striker who likes to drop deep and facilitate for Salah and Mane. SoccerMix picks up on this difference. Below we show the relative frequency that each player performs each prototypical shot, take-on, receival and pass. Actions shown in blue mean that Agüero performs the action more often than Firmino does, and those shown in red mean the opposite. Agüero indeed receives the ball more often close to the goal and is more likely to shoot. In contrast, Firmino is more active in receiving and passing the ball in the middle of the field.\nDefensive analysis: How did Liverpool force its opponents to deviate from their typical style? Analyzing defensive style is much harder as it involves off-the-ball actions such as correct positioning and putting pressure on attackers, which are not recorded in event streams. Our insight is that these off-the-ball actions are often performed with the intention of preventing certain actions from occurring. This suggests that we can gain a partial understanding of defensive style by measuring the effects that a team’s off-the-ball actions have on what on-the-ball actions their opponent performs. More precisely, we analyze how a team forces its opponents to deviate from their usual playing style. To illustrate this, we compare how teams play in their games (1) vs. Liverpool and (2) vs. all other opponents. Liverpool’s opponent is playing from left-to-right, and prototypical actions shown in blue (red) mean that teams perform these actions more (less) often when playing Liverpool than in their other matches.\nWhen playing Liverpool, teams tend to be flagged for offside more often than they typically are against other opponents. This indicates a well-synchronized line of defense that employs a very effective offside trap. The crosses show that, although Liverpool limits the number of crosses its opponents perform, this restriction is not symmetric: they allow fewer crosses from the left of defense (the offense’s right) than the right\nMatch analysis: Did Liverpool deviate from its usual style in the crucial away match against City? On January 3rd, 2019, Liverpool held a 6 point lead atop the EPL table when they traveled to play Manchester City in a highly anticipated match. Alas, in their only league loss of the season, Liverpool fell 2-1 and ended up missing out on the title to Manchester City by a single point. Using SoccerMix, we see that Manchester City had the front foot and forced Liverpool to play deeper in their own half:\nPrototypical actions shown in red (blue) indicate that Liverpool performed more (less) of these actions in this match than would be typical. We see that Liverpool has many more passes, dribbles, and receivals deep in their own half than is typical.\nThese findings are perhaps not surprising given Manchester City’s strength and the fact that the match took place at the Etihad. To dig deeper, we compare Liverpool’s playing style in this lost match to their earlier match versus Manchester City at Anfield, which they drew.\nWhen playing away, Liverpool made noticeably less use of its left flank, performing fewer passes, dribbles, and receivals in that area. This suggests that Liverpool’s left flank players were not functioning very well that match, which is further evidenced by midfielder James Milner and winger Sadio Mané on Liverpool’s left flank being substituted out in the 57th and 77th minute of the match.\nThis research was published at ECML/PKDD 2020 and the source code is available online. TD, MVR and JD did the scientific developments. PR helped with writing the blogpost and the visualizations. Data provided by StatsBomb.\n","date":1607212800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607212800,"objectID":"9b40ae2142e867ee29d6fba0b1733aa4","permalink":"https://ai.kuleuven.be/stories/post/2020-12-06-soccermix/","publishdate":"2020-12-06T00:00:00Z","relpermalink":"/stories/post/2020-12-06-soccermix/","section":"post","summary":"In this article, the DTAI Sports Analytics Lab is discussing their refined approach to analysing actions of soccer players. By learning common actions from data, they are able to identify a team's offensive or defensive strategies and, thus, create valuable insights for match-preparation. ","tags":["sports analytics","mixture models","data analytics","applications of AI"],"title":"SoccerMix: Representing soccer actions with mixture models","type":"post"},{"authors":["Jens Bürger"],"categories":["Leuven.AI"],"content":"Leuven.AI unites more than 75+ senior AI researchers across the whole university: its campuses, departments, faculties and research groups. Their research would not be possible though without the many junior AI researchers that work in their research groups. Recently, we organised our first Junior Researchers Event, aiming to bring together many of the young talents that will shape the development and application of AI in the coming years and decades in fields as diverse as public health, civil engineering, electrical engineering, computer science, astrology, psychology, human genetics, economics, law, linguistics, material science, mathematics, mechanical engineering, medicine, neuroscience, and computational biology, among others.\nThrough spotlight videos, junior researchers briefly introduced themselves and their research. These videos are a good representation of the breadth of AI research carried out at KU Leuven and that are articulated through the five strategic research lines of Leuven.AI. In this Leuven.AI Stories post, we want to feature the junior researchers that recorded brief spotlight videos to share with us a bit about themselves and their research interests.\nKU Leuven junior AI researchers (Click photos to view short videos presentations.)\n   Jan Czarnocki   Impact of AI and IoT on privacy and data protection.        Dries Debeer   Modelling learning behaviour in augmented and virtual realities.        André Tavares   Modal analysis and machine learning for structural dynamics.        Victor Verreet   Integrating continuous variables in hybrid probabilistic logic programming.        Pieter Delobelle   Natural language processing, fairness and bias, and their intersection.        Quinten Van Baelen   Integrating additional knowledge in traditional machine learning methods.        Elisabeth Heremans   Automated sleep staging based on wearable brain monitoring devices.        Zhenxiang Cao   Deep learning for change point detection and time series segmentation.        Jeroen Ooge   Human-Computer-Interfaces related to visual analytics and explainable AI.        Dejana Ugrenovic   Safety-critical, autonomous systems with ML components.        Peter Coppens   Data-driven dynamical systems identification and control.        Muzaffer Ayvaz   Global optimization of multivariate polynomials using tensor methods.        Jonas Doumens   Autonomous agents learning human-like languages.        Klaas Thoelen   Optimization of energy consumption through demand response.        Gavin Rens   Safety in reinforcement learning and optimization in MDP planning.        Matthias De Ryck   Migration from centralized to decentralized logistics systems.        Simon Vandevelde   User-friendly representation of domain knowledge with cDMN.        Pierre Carbonelle   Intelligent algorithms for legal applications.        Marjolein Deryck   Expert knowledge expressions and inference for problem solving.        Roman Kandinskii   Model compression for numerically efficient fluid dynamics modelling.        Jeroen Audenaert   Classification of stellar variability time-series.        Toon Vanderschueren   Cost-senstitive machine learning for business applications.        Arne Van De Kerchove   Development of brain-computer interfaces.        Thijs Peirelinck   Demand response optimization using reinforcement learning.        Suzanna Cuypers   Object detection for improving accuracy of concrete placement.        Jorik Jooken   Combinatorial optimisation interacting with machine learning.       Jan CzarnockiImpact of AI and IoT on privacy and data protection.      Dries DebeerModelling learning behaviour in augmented and virtual realities.      André TavaresModal analysis and machine learning for structural dynamics.      Victor VerreetIntegrating continuous variables in hybrid probabilistic logic programming.      Pieter DelobelleNatural language processing, fairness and bias, and their intersection.      Quinten Van BaelenIntegrating additional knowledge in traditional machine learning methods.      Elisabeth HeremansAutomated sleep staging based on wearable brain monitoring devices.      Zhenxiang CaoDeep learning for change point detection and time series segmentation.      Jeroen OogeHuman-Computer-Interfaces related to visual analytics and explainable AI.      Dejana UgrenovicSafety-critical, autonomous systems with ML components.      Peter CoppensData-driven dynamical systems identification and control.      Muzaffer AyvazGlobal optimization of multivariate polynomials using tensor methods.      Jonas DoumensAutonomous agents learning human-like languages.      Klaas ThoelenOptimization of energy consumption through demand response.      Gavin RensSafety in reinforcement learning and optimization in MDP planning.      Matthias De RyckMigration from centralized to decentralized logistics systems.      Simon VandeveldeUser-friendly representation of domain knowledge with cDMN.      Pierre CarbonelleIntelligent algorithms for legal applications.      Marjolein DeryckExpert knowledge expressions and inference for problem solving.      Roman KandinskiiModel compression for numerically efficient fluid dynamics modelling.      Jeroen AudenaertClassification of stellar variability time-series.      Toon VanderschuerenCost-senstitive machine learning for business applications.      Arne Van De KerchoveDevelopment of brain-computer interfaces.      Thijs PeirelinckDemand response optimization using reinforcement learning.      Suzanna CuypersObject detection for improving accuracy of concrete placement.      Jorik JookenCombinatorial optimisation interacting with machine learning.    -- ","date":1606435200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606435200,"objectID":"13e223a6dd00055dfe11c1016ee04837","permalink":"https://ai.kuleuven.be/stories/post/2020-11-27-jre/","publishdate":"2020-11-27T00:00:00Z","relpermalink":"/stories/post/2020-11-27-jre/","section":"post","summary":"Leuven.AI unites more than 75+ senior AI researchers across the whole university: its campuses, departments, faculties and research groups. Their research would not be possible though without the many junior AI researchers that work in their research groups.","tags":["Leuven.AI","networking","event","junior researchers event"],"title":"Meet Our Junior AI Researchers","type":"post"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://ai.kuleuven.be/stories/about/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/stories/about/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2dfd440454216e51db1490c3e8487f0b","permalink":"https://ai.kuleuven.be/stories/contributors/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/stories/contributors/","section":"","summary":"","tags":null,"title":"","type":"widget_page"}]